{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "level-sheep",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-tradition",
   "metadata": {},
   "source": [
    "#####  | Q-Learning algorithm [1]\n",
    "\n",
    "> $Q(s_{t}, a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t}, a_{t})}_{\\mathrm{old\\;value}}+\\underbrace{\\alpha}_{\\mathrm{learning\\;rate}} \\cdot \\left( \\overbrace{ \\underbrace{r_{t}}_{\\mathrm{reward}} + \\underbrace{\\gamma}_{\\mathrm{discount\\;factor}} \\cdot \\underbrace{\\underset{a_{t+1}}{\\rm{max}} Q(s_{t+1}, a_{t+1})}_{\\mathrm{estimate\\;of\\;optimal\\;future\\;value}} }^{\\mathrm{learned\\;value}} \\right )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-observation",
   "metadata": {},
   "source": [
    "---\n",
    "$s_{t}$ $\\rightarrow$ 시간 $t$ 에서 상태 (state)  \n",
    "$a_{t}$ $\\rightarrow$ 시간 $t$ 에서 행동 (action)\n",
    "$Q(s_{t}, a_{t})$ $\\rightarrow$ 주어진 상태 $s_{t}$ 에서 행동 $a_{t}$ 의 기대 Return (Reward)  \n",
    "$\\alpha$ $\\rightarrow$ 학습률 $(0 < \\alpha \\leqq 1)$  \n",
    "$r_{t}$ $\\rightarrow$ $a_{t}$ 에 의한 보상       \n",
    "$\\gamma$ $\\rightarrow$ 보상의 감소율 $(0 \\leqq \\gamma \\leqq 1)$  \n",
    "$\\underset{a_{t+1}}{\\rm{max}}Q(s_{t+1},a_{t+1})$ $\\rightarrow$ $Q$ 가 최대가 되는 $a_{t+1}$ 을 선택 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-inquiry",
   "metadata": {},
   "source": [
    "#####  | Greedy action \n",
    "항상 가장 좋은 action 을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-brunei",
   "metadata": {},
   "source": [
    "#####  | $\\epsilon-$greedy \n",
    "새로운 행동을 찾기 위한 무작위 선택, $\\epsilon$ 확률로 random 선택 $(0 \\leqq \\epsilon \\leqq 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-rogers",
   "metadata": {},
   "source": [
    "##### | Markov Decision Process\n",
    "Property #1 (Policy)\n",
    "> $P(a_{t+1}|s_{t}, a_{t}, s_{t+1})=P(a_{t+1}|s_{t+1})$  \n",
    "\n",
    "Property #2 (Transition probability)  \n",
    "> $P(s_{t+2}|s_{t}, a_{t}, s_{t+1}, a_{t+1})=P(s_{t+2}|s_{t+1}, a_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-missouri",
   "metadata": {},
   "source": [
    "##### | Return: Sum of rewards\n",
    "> $\n",
    "\\begin{aligned}\n",
    "G_{t} & \\triangleq R_{t} + \\gamma R_{t+1} + \\gamma^{2} R_{t+2} \\\\\n",
    "& = \\sum_{n=0}^{\\infty} \\gamma^{n} R_{t+n}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-wales",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[1] https://ko.wikipedia.org/wiki/Q_%EB%9F%AC%EB%8B%9D  \n",
    "[2] https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag  \n",
    "[3] https://www.youtube.com/user/hunkims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
