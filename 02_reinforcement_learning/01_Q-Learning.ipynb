{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-vancouver",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-flashing",
   "metadata": {},
   "source": [
    "#####  | Q-Learning algorithm [1]\n",
    "\n",
    "> $Q(s_{t}, a_{t}) \\leftarrow (1-\\alpha) \\cdot Q(s_{t}, a_{t})+ \\alpha \\cdot \\left[ r_{t} + \\gamma \\cdot \\underset{a_{t+1}}{\\rm{max}} Q(s_{t+1}, a_{t+1}) \\right ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-trailer",
   "metadata": {},
   "source": [
    "---\n",
    "$s_{t}$: State, 시간 $t$ 에서 상태  \n",
    "$a_{t}$: Action, 시간 $t$ 에서 행동  \n",
    "$Q(s_{t}, a_{t})$: Q function, 주어진 상태 ($s_{t}$) 와 행동 ($a_{t}$) 로 부터 Return의 기대값을 반환하는 함수    \n",
    "$\\alpha$:, Learning rate, 학습률 $(0 < \\alpha \\leqq 1)$  \n",
    "$r_{t}$: Reward, $a_{t}$ 에 의한 보상       \n",
    "$\\gamma$: Discount factor, 보상의 감소율 $(0 \\leqq \\gamma \\leqq 1)$  \n",
    "$\\underset{a_{t+1}}{\\rm{max}}Q(s_{t+1},a_{t+1})$: Greedy action, $Q$ 가 최대가 되도록 $a_{t+1}$ 선택 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-liabilities",
   "metadata": {},
   "source": [
    "#####  | Greedy action \n",
    "항상 가장 좋은 action 을 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-reading",
   "metadata": {},
   "source": [
    "#####  | $\\epsilon-$greedy \n",
    "새로운 행동을 찾기 위한 무작위 선택, $\\epsilon$ 확률로 random 선택 $(0 \\leqq \\epsilon \\leqq 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-journal",
   "metadata": {},
   "source": [
    "##### | Markov Decision Process\n",
    "Property #1 (Policy)\n",
    "> $P(a_{t+1}|s_{t}, a_{t}, s_{t+1})=P(a_{t+1}|s_{t+1})$  \n",
    "\n",
    "Property #2 (Transition probability)  \n",
    "> $P(s_{t+2}|s_{t}, a_{t}, s_{t+1}, a_{t+1})=P(s_{t+2}|s_{t+1}, a_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-impression",
   "metadata": {},
   "source": [
    "##### | Return: Sum of rewards\n",
    "> $\n",
    "\\begin{aligned}\n",
    "G_{t} & \\triangleq R_{t} + \\gamma R_{t+1} + \\gamma^{2} R_{t+2} \\\\\n",
    "& = \\sum_{n=0}^{\\infty} \\gamma^{n} R_{t+n}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-supply",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[1] https://ko.wikipedia.org/wiki/Q_%EB%9F%AC%EB%8B%9D  \n",
    "[2] https://www.youtube.com/channel/UCcbPAIfCa4q0x7x8yFXmBag  \n",
    "[3] https://www.youtube.com/user/hunkims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
